{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LwvvMtG18SK"
      },
      "source": [
        "# Recurrent Neural Network Homework\n",
        "\n",
        "This is the 4th assignment for CAP 4630 and we will implement a basic RNN network and an LSTM network with Keras to solve two problems. \\\n",
        "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total 100 points, including 15 bonus points)** \\\n",
        "You may use Machine Learning libaries like Scikit-learn for data preprocessing.\n",
        "\n",
        "**Task Overview:**\n",
        "- Implement a basic RNN network to solve time series prediction \n",
        "- Implement an LSTM network to conduct sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24oSrIK18SL"
      },
      "source": [
        "## 1 - Implement Basic RNN network with Keras to predict time series##\n",
        "### 1.1 Prepare the data (17 Points)\n",
        "\n",
        "Prepare time series data for deep neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the given train and test data: \"train.txt\" and \"test.txt\". **(5 Points)**\n",
        "2. Generate the **TRAIN** and **TEST** labels. **(5 Points)**\n",
        "3. Normalize the **TRAIN** and **TEST** data with sklearn function \"MinMaxScaler\". **(5 Points)**\n",
        "4. **PRINT OUT** the **TEST** data and label. **(2 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. The length of original train data is 113 which starts from **\"1949-01\"** to **\"1958-05\"**. The length of original test data is 29, which starts from **\"1958-07\"** to **\"1960-11\"**. \n",
        "2. Set the data types of both train and test data to \"float32\". \n",
        "3. When you prepared input data X (sequences) and oupt data Y (labels), please consider the following relationship:\n",
        "    - The sequence X should be the **past 12** datapoints in the time series, i.e., observation sequence with historical window of 12. You may check the time series data and think about the reason.\n",
        "    - The label Y should be the **next 1** datapoint in the time series (one point ahead prediction).\n",
        "4. The first 3 **TRAIN** data and label should be:\n",
        "\n",
        "- trainX[0] = [[0.02203858 &nbsp; 0.03856748 &nbsp; 0.077135 &nbsp;  0.06887051 &nbsp; 0.04683197 &nbsp; 0.08539945 &nbsp; 0.12121212 &nbsp; 0.12121212 &nbsp; 0.08815429 &nbsp; 0.04132232 &nbsp; 0.    &nbsp; 0.03856748]]\n",
        "- trainY[0] = [0.03030303]\n",
        "\n",
        "- trianX[1] = [[0.03856748 &nbsp; 0.077135 &nbsp;  0.06887051 &nbsp; 0.04683197  &nbsp; 0.08539945  &nbsp; 0.12121212  &nbsp; 0.12121212  &nbsp; 0.08815429  &nbsp; 0.04132232  &nbsp; 0.     &nbsp;  0.03856748   &nbsp; 0.03030303]]\n",
        "- trainY[1] = [0.06060606]\n",
        "\n",
        "- trainX[2] =  [[0.077135 &nbsp;  0.06887051 &nbsp; 0.04683197 &nbsp; 0.08539945 &nbsp; 0.12121212 &nbsp; 0.12121212 &nbsp; 0.08815429 &nbsp; 0.04132232 &nbsp; 0.    &nbsp;     0.03856748 &nbsp; 0.03030303 &nbsp; 0.06060606]]\n",
        "- trainY[2] = [0.10192838]\n",
        "\n",
        "5. Apply the MinMaxScaler to both the train and test data.\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "\n",
        "\n",
        "6. After the preparation with scaler fitting, the shapes of trainX, trainY, testX, and testY are as follows:\\\n",
        "trainX.shape = (101, 1, 12)\\\n",
        "trainY.shape = (101,)\\\n",
        "testX.shape = (17, 1, 12)\\\n",
        "testY.shape = (17,)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS7xhrC3-_-1"
      },
      "source": [
        "### Import Libraries ###\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers \n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "reset_random_seeds()"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shAj2Y6IuxUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2593a66-a7ac-4ab5-a670-8fc87fb9f1b5"
      },
      "source": [
        "### Prepare and Preprocess Data Here ###\n",
        "from pandas import read_csv\n",
        "\n",
        "train_data = read_csv('train.txt')\n",
        "test_data = read_csv('test.txt')\n",
        "\n",
        "### Design a Function to Prepare Observation Sequences and Corresponding Labels ###\n",
        "def create_dataset(dataset, look_back=12): # look_back is used to specify input sequence length\n",
        "    dataX, dataY = [], []\n",
        "    i = 0\n",
        "    for i in range(len(dataset)-look_back):\n",
        "        dataX.append(dataset[i:i+look_back]) \n",
        "        dataY.append(dataset[i+look_back])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "\n",
        "### Train and Test Data Loading with float32 type ####\n",
        "dataframe_train = read_csv('train.txt', usecols=[1], engine='python') # Read train.txt \n",
        "dataset_train = dataframe_train.values\n",
        "dataset_train = dataset_train.astype('float32') # Specify the data type to 'float32'\n",
        "\n",
        "dataframe_test = read_csv('test.txt', usecols=[1], engine='python') # Read test.txt \n",
        "dataset_test = dataframe_test.values\n",
        "dataset_test = dataset_test.astype('float32') # Specify the data type to 'float32'\n",
        "\n",
        "\n",
        "\n",
        "### Scale Training and Test Data to [0, 1] ###\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1)) # specify the scaler\n",
        "train = scaler.fit_transform(dataset_train) # fit the scaler to the training data\n",
        "test = scaler.fit_transform(dataset_test) # fit the scaler to the test data\n",
        "\n",
        "### Training and Test Data Split ###\n",
        "trainX, trainY = create_dataset(train)\n",
        "testX, testY = create_dataset(test)\n",
        "\n",
        "\n",
        "### Training and Test Data Reshape (to fit RNN input) ###\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "\n",
        "trainY = np.reshape(trainY, (trainY.shape[0],))\n",
        "testY = np.reshape(testY, (testY.shape[0],))\n",
        "\n",
        "print('Shapes:')\n",
        "print('trainX', trainX.shape)\n",
        "print('trainY', trainY.shape)\n",
        "print('testX', testX.shape)\n",
        "print('testY', testY.shape)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "trainX (101, 1, 12)\n",
            "trainY (101,)\n",
            "testX (17, 1, 12)\n",
            "testY (17,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ1uHWzX8wlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bc0eea-84da-4bf9-8585-c5dde8384eb2"
      },
      "source": [
        "### Print Out the TEST Data and Labels Here ###\n",
        "print('textX:')\n",
        "print(testX)\n",
        "print('\\n\\ntestY:')\n",
        "print(testY)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "textX:\n",
            "[[[0.5801282  0.625      0.30128205 0.15705132 0.         0.08653843\n",
            "   0.16025639 0.1025641  0.3076923  0.27564108 0.3525641  0.5192307 ]]\n",
            "\n",
            " [[0.625      0.30128205 0.15705132 0.         0.08653843 0.16025639\n",
            "   0.1025641  0.3076923  0.27564108 0.3525641  0.5192307  0.7628205 ]]\n",
            "\n",
            " [[0.30128205 0.15705132 0.         0.08653843 0.16025639 0.1025641\n",
            "   0.3076923  0.27564108 0.3525641  0.5192307  0.7628205  0.798077  ]]\n",
            "\n",
            " [[0.15705132 0.         0.08653843 0.16025639 0.1025641  0.3076923\n",
            "   0.27564108 0.3525641  0.5192307  0.7628205  0.798077   0.49038458]]\n",
            "\n",
            " [[0.         0.08653843 0.16025639 0.1025641  0.3076923  0.27564108\n",
            "   0.3525641  0.5192307  0.7628205  0.798077   0.49038458 0.31089747]]\n",
            "\n",
            " [[0.08653843 0.16025639 0.1025641  0.3076923  0.27564108 0.3525641\n",
            "   0.5192307  0.7628205  0.798077   0.49038458 0.31089747 0.16666663]]\n",
            "\n",
            " [[0.16025639 0.1025641  0.3076923  0.27564108 0.3525641  0.5192307\n",
            "   0.7628205  0.798077   0.49038458 0.31089747 0.16666663 0.30448723]]\n",
            "\n",
            " [[0.1025641  0.3076923  0.27564108 0.3525641  0.5192307  0.7628205\n",
            "   0.798077   0.49038458 0.31089747 0.16666663 0.30448723 0.34294868]]\n",
            "\n",
            " [[0.3076923  0.27564108 0.3525641  0.5192307  0.7628205  0.798077\n",
            "   0.49038458 0.31089747 0.16666663 0.30448723 0.34294868 0.25961542]]\n",
            "\n",
            " [[0.27564108 0.3525641  0.5192307  0.7628205  0.798077   0.49038458\n",
            "   0.31089747 0.16666663 0.30448723 0.34294868 0.25961542 0.34935904]]\n",
            "\n",
            " [[0.3525641  0.5192307  0.7628205  0.798077   0.49038458 0.31089747\n",
            "   0.16666663 0.30448723 0.34294868 0.25961542 0.34935904 0.48397434]]\n",
            "\n",
            " [[0.5192307  0.7628205  0.798077   0.49038458 0.31089747 0.16666663\n",
            "   0.30448723 0.34294868 0.25961542 0.34935904 0.48397434 0.5192307 ]]\n",
            "\n",
            " [[0.7628205  0.798077   0.49038458 0.31089747 0.16666663 0.30448723\n",
            "   0.34294868 0.25961542 0.34935904 0.48397434 0.5192307  0.72115386]]\n",
            "\n",
            " [[0.798077   0.49038458 0.31089747 0.16666663 0.30448723 0.34294868\n",
            "   0.25961542 0.34935904 0.48397434 0.5192307  0.72115386 1.        ]]\n",
            "\n",
            " [[0.49038458 0.31089747 0.16666663 0.30448723 0.34294868 0.25961542\n",
            "   0.34935904 0.48397434 0.5192307  0.72115386 1.         0.94871795]]\n",
            "\n",
            " [[0.31089747 0.16666663 0.30448723 0.34294868 0.25961542 0.34935904\n",
            "   0.48397434 0.5192307  0.72115386 1.         0.94871795 0.6346154 ]]\n",
            "\n",
            " [[0.16666663 0.30448723 0.34294868 0.25961542 0.34935904 0.48397434\n",
            "   0.5192307  0.72115386 1.         0.94871795 0.6346154  0.48397434]]]\n",
            "\n",
            "\n",
            "testY:\n",
            "[0.7628205  0.798077   0.49038458 0.31089747 0.16666663 0.30448723\n",
            " 0.34294868 0.25961542 0.34935904 0.48397434 0.5192307  0.72115386\n",
            " 1.         0.94871795 0.6346154  0.48397434 0.25641024]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AlakqA_vuFb"
      },
      "source": [
        "### 1.2 - Build the RNN model (20 Points) ##\n",
        "\n",
        "\n",
        "Build an RNN model with SimpleRNN cell. \n",
        "\n",
        "**Tasks:**\n",
        "1. Build an RNN model with 1 simple RNN layer and 1 Dense layer.  **(10 Points)**\n",
        "2. Compile the model. **(5 Points)**\n",
        "3. Train the model for **1000** epochs with **batch_size = 10**. **(5 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may consider **tensorflow.keras.layers.SimpleRNN(unit_size=4)** to specify RNN cells.\n",
        "2. Use loss function = 'mean_squared_error' and select **Adam** optimizer with **learning_rate=0.005** and other default settings.\n",
        "3. After first epoch, the train loss is changed to around **0.0912**. \n",
        "4. The model summary is as follows:\n",
        "- Total params: 73\n",
        "- Trainable params: 73\n",
        "- Non-trainable params: 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn92qh8oyq0B"
      },
      "source": [
        "### Build the RNN Model ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "\n",
        "epochs = 1000\n",
        "batch_size = 10\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model = Sequential() # Declare Sequential class and assign it to variable \"model\"\n",
        "model.add(keras.layers.SimpleRNN(4)) # Add a simple RNN layer with unit_size=4 in the model \n",
        "model.add(keras.layers.Dense(1)) # Add a following Dense layer with units=1 in the model "
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnO-5WT-3hgH"
      },
      "source": [
        "### Compile the RNN Model  ###\n",
        "\n",
        "opt = 'adam' #tf.optimizers.Adam(learning_rate=0.005)\n",
        "mse_loss = 'mean_squared_error'\n",
        "model.compile(optimizer=opt, loss=mse_loss) # model compiled with mean_squared_error loss and adam optimizer"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tpZAutlzify",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7fd8efe-036e-4370-cd5e-34c2bfa2083d"
      },
      "source": [
        "### Train the RNN Model  ###\n",
        "\n",
        "model.fit(x=trainX, y=trainY, batch_size=batch_size, epochs=epochs)# model fit with epoch=1000, batch_size=10; verbose=2 is optional.\n",
        "model.summary() # print out model structure with model.summary()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "11/11 [==============================] - 1s 5ms/step - loss: 0.1754\n",
            "Epoch 2/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1121\n",
            "Epoch 3/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0695\n",
            "Epoch 4/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0433\n",
            "Epoch 5/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0317\n",
            "Epoch 6/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0268\n",
            "Epoch 7/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0246\n",
            "Epoch 8/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0240\n",
            "Epoch 9/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0235\n",
            "Epoch 10/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0230\n",
            "Epoch 11/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0226\n",
            "Epoch 12/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0218\n",
            "Epoch 13/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0216\n",
            "Epoch 14/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0212\n",
            "Epoch 15/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0206\n",
            "Epoch 16/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0201\n",
            "Epoch 17/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0197\n",
            "Epoch 18/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0194\n",
            "Epoch 19/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0191\n",
            "Epoch 20/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0187\n",
            "Epoch 21/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0183\n",
            "Epoch 22/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0178\n",
            "Epoch 23/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0174\n",
            "Epoch 24/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0171\n",
            "Epoch 25/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0167\n",
            "Epoch 26/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0163\n",
            "Epoch 27/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0159\n",
            "Epoch 28/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0157\n",
            "Epoch 29/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0153\n",
            "Epoch 30/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0148\n",
            "Epoch 31/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0146\n",
            "Epoch 32/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0143\n",
            "Epoch 33/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0139\n",
            "Epoch 34/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0137\n",
            "Epoch 35/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0133\n",
            "Epoch 36/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0130\n",
            "Epoch 37/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0128\n",
            "Epoch 38/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0125\n",
            "Epoch 39/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0125\n",
            "Epoch 40/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0121\n",
            "Epoch 41/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0116\n",
            "Epoch 42/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0114\n",
            "Epoch 43/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0111\n",
            "Epoch 44/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0109\n",
            "Epoch 45/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0108\n",
            "Epoch 46/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0106\n",
            "Epoch 47/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0102\n",
            "Epoch 48/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0100\n",
            "Epoch 49/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0098\n",
            "Epoch 50/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0095\n",
            "Epoch 51/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0095\n",
            "Epoch 52/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0091\n",
            "Epoch 53/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0089\n",
            "Epoch 54/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0087\n",
            "Epoch 55/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0086\n",
            "Epoch 56/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0083\n",
            "Epoch 57/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0081\n",
            "Epoch 58/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0079\n",
            "Epoch 59/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0078\n",
            "Epoch 60/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0076\n",
            "Epoch 61/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0075\n",
            "Epoch 62/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0073\n",
            "Epoch 63/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 64/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0068\n",
            "Epoch 65/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0068\n",
            "Epoch 66/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0066\n",
            "Epoch 67/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0065\n",
            "Epoch 68/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0063\n",
            "Epoch 69/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0061\n",
            "Epoch 70/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0060\n",
            "Epoch 71/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0058\n",
            "Epoch 72/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0057\n",
            "Epoch 73/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0056\n",
            "Epoch 74/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 75/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0054\n",
            "Epoch 76/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0052\n",
            "Epoch 77/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0052\n",
            "Epoch 78/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0050\n",
            "Epoch 79/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0050\n",
            "Epoch 80/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 81/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0048\n",
            "Epoch 82/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0049\n",
            "Epoch 83/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0047\n",
            "Epoch 84/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0046\n",
            "Epoch 85/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0046\n",
            "Epoch 86/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0043\n",
            "Epoch 87/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0043\n",
            "Epoch 88/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0043\n",
            "Epoch 89/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0041\n",
            "Epoch 90/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 91/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 92/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0040\n",
            "Epoch 93/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 94/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 95/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0037\n",
            "Epoch 96/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0036\n",
            "Epoch 97/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 98/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0036\n",
            "Epoch 99/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0035\n",
            "Epoch 100/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0035\n",
            "Epoch 101/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0034\n",
            "Epoch 102/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0034\n",
            "Epoch 103/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0034\n",
            "Epoch 104/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 105/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 106/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 107/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 108/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 109/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 110/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 111/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0030\n",
            "Epoch 112/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0030\n",
            "Epoch 113/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 114/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 115/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 116/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 117/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 118/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 119/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 120/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0027\n",
            "Epoch 121/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0028\n",
            "Epoch 122/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0027\n",
            "Epoch 123/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0027\n",
            "Epoch 124/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0027\n",
            "Epoch 125/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 126/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 127/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 128/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 129/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 130/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 131/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 132/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 133/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 134/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 135/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 136/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 137/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 138/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 139/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 140/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 141/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 142/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 143/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 144/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 145/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 146/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 147/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 148/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 149/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 150/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 151/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 152/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 153/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 154/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 155/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 156/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 157/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 158/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 159/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 160/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 161/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 162/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 163/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 164/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 165/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 166/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 167/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 168/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 169/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 170/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 171/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 172/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 173/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 174/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 175/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 176/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 177/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 178/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 179/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 180/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 181/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 182/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 183/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 184/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 185/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 186/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 187/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 188/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 189/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0017\n",
            "Epoch 190/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 191/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0017\n",
            "Epoch 192/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 193/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 194/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 195/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 196/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 197/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 198/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 199/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 200/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 201/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0018\n",
            "Epoch 202/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 203/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 204/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 205/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 206/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 207/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 208/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 209/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 210/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 211/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 212/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 213/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 214/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 215/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 216/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 217/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 218/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 219/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 220/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 221/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 222/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 223/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 224/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 225/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 226/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 227/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 228/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 229/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 230/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 231/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 232/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 233/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 234/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 235/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 236/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 237/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 238/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 239/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 240/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 241/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 242/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 243/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 244/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 245/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 246/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 247/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 248/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 249/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 250/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 251/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 252/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 253/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 254/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 255/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 256/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 257/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 258/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 259/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 260/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 261/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 262/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 263/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 264/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 265/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 266/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 267/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 268/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 269/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 270/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 271/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 272/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 273/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 274/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 275/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 276/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 277/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 278/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 279/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 280/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 281/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 282/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 283/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 284/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 285/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 286/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 287/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 288/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 289/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 290/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 291/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 292/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 293/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 294/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 295/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 296/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 297/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 298/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 299/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 300/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 301/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 302/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 303/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 304/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 305/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 306/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 307/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 308/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 309/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 310/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 311/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 312/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 313/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 314/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 315/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 316/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 317/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 318/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 319/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 320/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 321/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 322/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 323/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 324/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 325/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 326/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 327/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 328/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 329/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 330/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 331/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 332/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 333/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 334/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 335/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 336/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 337/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 338/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 339/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 340/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 341/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 342/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 343/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 344/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 345/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 346/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 347/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 348/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 349/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 350/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 351/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 352/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 353/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 354/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 355/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 356/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 357/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 358/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 359/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 360/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 361/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 362/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 363/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 364/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 365/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 366/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 367/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 368/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 369/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 370/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 371/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 372/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 373/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 374/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 375/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 376/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 377/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 378/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 379/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 380/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 381/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 382/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 383/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 384/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 385/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 386/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 387/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 388/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 389/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 390/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 391/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 392/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 393/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 394/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 395/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 396/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 397/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 398/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 399/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 400/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 401/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 402/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 403/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 404/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 405/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 406/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 407/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 408/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 409/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 410/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 411/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 412/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 413/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 414/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 415/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 416/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 417/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 418/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 419/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 420/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 421/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 422/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 423/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 424/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 425/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 426/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 427/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 428/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 429/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 430/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 431/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 432/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 433/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 434/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 435/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 436/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 437/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 438/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 439/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 440/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 441/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 442/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 443/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 444/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 445/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 446/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 447/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 448/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 449/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 450/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 451/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 452/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 453/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 454/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 455/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 456/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 457/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 458/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 459/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 460/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 461/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 462/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 463/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 464/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 465/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 466/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 467/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 468/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 469/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 470/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 471/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 472/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 473/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 474/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 475/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 476/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 477/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 478/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 479/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 480/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 481/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 482/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 483/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 484/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 485/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 486/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 487/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 488/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 489/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 490/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 491/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 492/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 493/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 494/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 495/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 496/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 497/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 498/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 499/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 500/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 501/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 502/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 503/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 504/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 505/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 506/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 507/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 508/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 509/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 510/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 511/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 512/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 513/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 514/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 515/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 516/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 517/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 518/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 519/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 520/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 521/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 522/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 523/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 524/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 525/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 526/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 527/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 528/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 529/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 530/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 531/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 532/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 533/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 534/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 535/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 536/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 537/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 538/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 539/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 540/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 541/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 542/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 543/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 544/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 545/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 546/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 547/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 548/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 549/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 550/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 551/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 552/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 553/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 554/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 555/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 556/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 557/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 558/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 559/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 560/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 561/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 562/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 563/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 564/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 565/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 566/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 567/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 568/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 569/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 570/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 571/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 572/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 573/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 574/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 575/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 576/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 577/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 578/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 579/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 580/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 581/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 582/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 583/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 584/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 585/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 586/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 587/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 588/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 589/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 590/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 591/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010\n",
            "Epoch 592/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 593/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 594/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 595/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 596/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 597/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 598/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 599/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 600/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 601/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 602/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 603/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 604/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 605/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 606/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 607/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 608/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 609/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 610/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 611/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 612/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 613/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 614/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 615/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8034e-04\n",
            "Epoch 616/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 617/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 618/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 619/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 620/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 621/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 622/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 623/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 624/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 625/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 626/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 627/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 628/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 629/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 630/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 631/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 632/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 633/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9775e-04\n",
            "Epoch 634/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 635/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 636/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9992e-04\n",
            "Epoch 637/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 638/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 639/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 640/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 641/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9263e-04\n",
            "Epoch 642/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 643/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 644/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 645/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 646/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7487e-04\n",
            "Epoch 647/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9633e-04\n",
            "Epoch 648/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8808e-04\n",
            "Epoch 649/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 650/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 651/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 652/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 653/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 654/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 655/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 656/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010\n",
            "Epoch 657/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 658/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 659/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 660/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7245e-04\n",
            "Epoch 661/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 662/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8960e-04\n",
            "Epoch 663/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9013e-04\n",
            "Epoch 664/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 665/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 666/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.9253e-04\n",
            "Epoch 667/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 668/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 669/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8596e-04\n",
            "Epoch 670/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 671/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 672/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.9299e-04\n",
            "Epoch 673/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 674/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9716e-04\n",
            "Epoch 675/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 676/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 677/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 678/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 679/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 680/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 681/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 682/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 683/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 684/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9851e-04\n",
            "Epoch 685/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8619e-04\n",
            "Epoch 686/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 687/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 688/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 689/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7566e-04\n",
            "Epoch 690/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 691/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8072e-04\n",
            "Epoch 692/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 693/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010\n",
            "Epoch 694/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0013\n",
            "Epoch 695/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 696/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 697/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 698/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 699/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9416e-04\n",
            "Epoch 700/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 701/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7986e-04\n",
            "Epoch 702/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8860e-04\n",
            "Epoch 703/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9580e-04\n",
            "Epoch 704/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7624e-04\n",
            "Epoch 705/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9562e-04\n",
            "Epoch 706/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9606e-04\n",
            "Epoch 707/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 708/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 709/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 710/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7979e-04\n",
            "Epoch 711/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 712/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 713/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 714/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9297e-04\n",
            "Epoch 715/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9640e-04\n",
            "Epoch 716/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9770e-04\n",
            "Epoch 717/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8507e-04\n",
            "Epoch 718/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 719/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010\n",
            "Epoch 720/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7291e-04\n",
            "Epoch 721/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8761e-04\n",
            "Epoch 722/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 723/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8339e-04\n",
            "Epoch 724/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9459e-04\n",
            "Epoch 725/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 726/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 727/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.7461e-04\n",
            "Epoch 728/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 729/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 730/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7493e-04\n",
            "Epoch 731/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 732/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 733/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6942e-04\n",
            "Epoch 734/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7017e-04\n",
            "Epoch 735/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8097e-04\n",
            "Epoch 736/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9289e-04\n",
            "Epoch 737/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 738/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7788e-04\n",
            "Epoch 739/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 740/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6616e-04\n",
            "Epoch 741/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 742/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 743/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 744/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 745/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 746/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 747/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9571e-04\n",
            "Epoch 748/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8448e-04\n",
            "Epoch 749/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 750/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9259e-04\n",
            "Epoch 751/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6440e-04\n",
            "Epoch 752/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7614e-04\n",
            "Epoch 753/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6923e-04\n",
            "Epoch 754/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7538e-04\n",
            "Epoch 755/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9156e-04\n",
            "Epoch 756/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 757/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 758/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 759/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 760/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 761/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9513e-04\n",
            "Epoch 762/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 763/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7166e-04\n",
            "Epoch 764/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7004e-04\n",
            "Epoch 765/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8616e-04\n",
            "Epoch 766/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7818e-04\n",
            "Epoch 767/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7474e-04\n",
            "Epoch 768/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7922e-04\n",
            "Epoch 769/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 770/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 771/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 772/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8542e-04\n",
            "Epoch 773/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9623e-04\n",
            "Epoch 774/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 775/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 776/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 777/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 778/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6708e-04\n",
            "Epoch 779/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9758e-04\n",
            "Epoch 780/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8337e-04\n",
            "Epoch 781/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9721e-04\n",
            "Epoch 782/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7326e-04\n",
            "Epoch 783/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9239e-04\n",
            "Epoch 784/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6735e-04\n",
            "Epoch 785/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6704e-04\n",
            "Epoch 786/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9506e-04\n",
            "Epoch 787/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 788/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8740e-04\n",
            "Epoch 789/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6278e-04\n",
            "Epoch 790/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7644e-04\n",
            "Epoch 791/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5962e-04\n",
            "Epoch 792/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6755e-04\n",
            "Epoch 793/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 794/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 795/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6417e-04\n",
            "Epoch 796/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010\n",
            "Epoch 797/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7034e-04\n",
            "Epoch 798/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8346e-04\n",
            "Epoch 799/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.7853e-04\n",
            "Epoch 800/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7710e-04\n",
            "Epoch 801/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8059e-04\n",
            "Epoch 802/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5904e-04\n",
            "Epoch 803/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 804/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 805/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8580e-04\n",
            "Epoch 806/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 807/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 808/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.9493e-04\n",
            "Epoch 809/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6211e-04\n",
            "Epoch 810/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6582e-04\n",
            "Epoch 811/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7023e-04\n",
            "Epoch 812/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6668e-04\n",
            "Epoch 813/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5158e-04\n",
            "Epoch 814/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8115e-04\n",
            "Epoch 815/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8101e-04\n",
            "Epoch 816/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 817/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8709e-04\n",
            "Epoch 818/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6471e-04\n",
            "Epoch 819/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5769e-04\n",
            "Epoch 820/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6198e-04\n",
            "Epoch 821/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.6128e-04\n",
            "Epoch 822/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.5440e-04\n",
            "Epoch 823/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 824/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7200e-04\n",
            "Epoch 825/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 826/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8592e-04\n",
            "Epoch 827/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6126e-04\n",
            "Epoch 828/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8046e-04\n",
            "Epoch 829/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 830/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9096e-04\n",
            "Epoch 831/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9955e-04\n",
            "Epoch 832/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7404e-04\n",
            "Epoch 833/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.6452e-04\n",
            "Epoch 834/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8955e-04\n",
            "Epoch 835/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 836/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 837/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 838/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9614e-04\n",
            "Epoch 839/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9123e-04\n",
            "Epoch 840/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9089e-04\n",
            "Epoch 841/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4928e-04\n",
            "Epoch 842/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7750e-04\n",
            "Epoch 843/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8770e-04\n",
            "Epoch 844/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 845/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0015\n",
            "Epoch 846/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 847/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6700e-04\n",
            "Epoch 848/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 849/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0014\n",
            "Epoch 850/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 851/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 852/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 853/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.9927e-04\n",
            "Epoch 854/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 855/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 856/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6436e-04\n",
            "Epoch 857/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8122e-04\n",
            "Epoch 858/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.9309e-04\n",
            "Epoch 859/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9846e-04\n",
            "Epoch 860/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 861/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9472e-04\n",
            "Epoch 862/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010\n",
            "Epoch 863/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8424e-04\n",
            "Epoch 864/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6950e-04\n",
            "Epoch 865/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.6207e-04\n",
            "Epoch 866/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 867/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 868/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 869/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5043e-04\n",
            "Epoch 870/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.8985e-04\n",
            "Epoch 871/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6641e-04\n",
            "Epoch 872/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 873/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 874/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5390e-04\n",
            "Epoch 875/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6330e-04\n",
            "Epoch 876/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6955e-04\n",
            "Epoch 877/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9433e-04\n",
            "Epoch 878/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6041e-04\n",
            "Epoch 879/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9003e-04\n",
            "Epoch 880/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6611e-04\n",
            "Epoch 881/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7048e-04\n",
            "Epoch 882/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7569e-04\n",
            "Epoch 883/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 884/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9437e-04\n",
            "Epoch 885/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7743e-04\n",
            "Epoch 886/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6381e-04\n",
            "Epoch 887/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 888/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7927e-04\n",
            "Epoch 889/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 890/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 891/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 892/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5494e-04\n",
            "Epoch 893/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6554e-04\n",
            "Epoch 894/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7959e-04\n",
            "Epoch 895/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9793e-04\n",
            "Epoch 896/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 897/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 898/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 899/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 900/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6084e-04\n",
            "Epoch 901/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4633e-04\n",
            "Epoch 902/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5850e-04\n",
            "Epoch 903/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5021e-04\n",
            "Epoch 904/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 905/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4850e-04\n",
            "Epoch 906/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.5940e-04\n",
            "Epoch 907/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4216e-04\n",
            "Epoch 908/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4666e-04\n",
            "Epoch 909/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4998e-04\n",
            "Epoch 910/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5692e-04\n",
            "Epoch 911/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6802e-04\n",
            "Epoch 912/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 913/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6929e-04\n",
            "Epoch 914/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 915/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010\n",
            "Epoch 916/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 917/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 918/1000\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 9.4625e-04\n",
            "Epoch 919/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6663e-04\n",
            "Epoch 920/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4443e-04\n",
            "Epoch 921/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9114e-04\n",
            "Epoch 922/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5021e-04\n",
            "Epoch 923/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4354e-04\n",
            "Epoch 924/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 925/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4582e-04\n",
            "Epoch 926/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4784e-04\n",
            "Epoch 927/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6674e-04\n",
            "Epoch 928/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7444e-04\n",
            "Epoch 929/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.5373e-04\n",
            "Epoch 930/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 931/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5109e-04\n",
            "Epoch 932/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4534e-04\n",
            "Epoch 933/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4243e-04\n",
            "Epoch 934/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6315e-04\n",
            "Epoch 935/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010\n",
            "Epoch 936/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 937/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.3424e-04\n",
            "Epoch 938/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 939/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9399e-04\n",
            "Epoch 940/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 941/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9138e-04\n",
            "Epoch 942/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 943/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5197e-04\n",
            "Epoch 944/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.5846e-04\n",
            "Epoch 945/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7658e-04\n",
            "Epoch 946/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6296e-04\n",
            "Epoch 947/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.9296e-04\n",
            "Epoch 948/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4606e-04\n",
            "Epoch 949/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6595e-04\n",
            "Epoch 950/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 951/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 952/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5959e-04\n",
            "Epoch 953/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.6657e-04\n",
            "Epoch 954/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6642e-04\n",
            "Epoch 955/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6070e-04\n",
            "Epoch 956/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4114e-04\n",
            "Epoch 957/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5426e-04\n",
            "Epoch 958/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5566e-04\n",
            "Epoch 959/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 960/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 961/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 962/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 963/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5761e-04\n",
            "Epoch 964/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 965/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9799e-04\n",
            "Epoch 966/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.4722e-04\n",
            "Epoch 967/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9045e-04\n",
            "Epoch 968/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8109e-04\n",
            "Epoch 969/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 970/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9489e-04\n",
            "Epoch 971/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5633e-04\n",
            "Epoch 972/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 973/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.6646e-04\n",
            "Epoch 974/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7739e-04\n",
            "Epoch 975/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 976/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.5711e-04\n",
            "Epoch 977/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6627e-04\n",
            "Epoch 978/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6789e-04\n",
            "Epoch 979/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 980/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4799e-04\n",
            "Epoch 981/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4507e-04\n",
            "Epoch 982/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5236e-04\n",
            "Epoch 983/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4131e-04\n",
            "Epoch 984/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 985/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 986/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8261e-04\n",
            "Epoch 987/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4148e-04\n",
            "Epoch 988/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8310e-04\n",
            "Epoch 989/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.7086e-04\n",
            "Epoch 990/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4246e-04\n",
            "Epoch 991/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.4609e-04\n",
            "Epoch 992/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6748e-04\n",
            "Epoch 993/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4965e-04\n",
            "Epoch 994/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 995/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7032e-04\n",
            "Epoch 996/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 997/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9928e-04\n",
            "Epoch 998/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.4567e-04\n",
            "Epoch 999/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5828e-04\n",
            "Epoch 1000/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.5735e-04\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 4)                 68        \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 73\n",
            "Trainable params: 73\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd2jZl4n0H8m"
      },
      "source": [
        "### 1.3 Evaluate Predictive Model Performance (10 Points)\n",
        "\n",
        "Predict datapoints with the observed datapoints and trained model. \n",
        "\n",
        "**Tasks:**\n",
        "1. Do direct prediction on train and test datapoints with the obtained model in section 1.2. **(2 Points)**\n",
        "2. Scale the prediction results back to original representation with the scaler.(scaler.inverse_transform function) **(3 Points)**\n",
        "3. Calculate root mean squared error (RMSE) and **print out** the error for **both TRAIN and TEST**. **(3 Points)**\n",
        "4. **Plot** the **TEST** label and prediction. **(2 Points)**\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Scale back the predictions with the build-in function \"scaler.inverse_transform\".\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform\n",
        "2. For validation: Train Score: **~10.92 RMSE** Test Score: **~27.70 RMSE**\n",
        "3. The plot for validation is shown below (observation test data are blue and prediction results are orange):\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=10c1Fsa_9v0AQf2fDpCzGPFPxbIEso81u)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNEkAMxnz8Mq"
      },
      "source": [
        "### Make Predictions ###\n",
        "\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbnRqEv9z-he"
      },
      "source": [
        "### Scale Back Predictions ###\n",
        "\n",
        "trainPredict = scaler.inverse_transform(trainPredict) # scale train prediction back with scaler.inverse_transform()\n",
        "trainY = scaler.inverse_transform([trainY])  # scale train labels back with scaler.inverse_transform()\n",
        "\n",
        "testPredict = scaler.inverse_transform(testPredict) # scale test prediction back with scaler.inverse_transform()\n",
        "testY = scaler.inverse_transform([testY]) # scale test labels back with scaler.inverse_transform()\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdBWzmE91G6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23823662-b199-48f7-9d91-f1df3d2773d6"
      },
      "source": [
        "### Calculate Root Mean Squared Error (RMSE) ###\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error # Import mean_squared_error from sklearn.metrics\n",
        "\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0])) \n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Score: 9.61 RMSE\n",
            "Test Score: 23.05 RMSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txdu8q7l1aju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5bb30c9e-da14-45ff-c545-4567902ca251"
      },
      "source": [
        "### Plot Observation Data and Prediction Results with TEST dataset ###\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(testY[0]) # Plot Observations in Test Set\n",
        "plt.plot(testPredict) # Plot Predictions in Test Set\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O49Ug-FhtCg8"
      },
      "source": [
        "## 2 - Build an LSTM model to conduct sentiment analysis ##\n",
        "\n",
        "### 2.1 Prepare the data (13 Points) ###\n",
        "\n",
        "Prepare IMDB data for reccurent neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the data from IMDB review dataset and **print out** the lengths of sequences. **(3 Points)**\n",
        "2. Preprocess review data to meet the network input requirement by specifying **number of words=1000**, setting **the analysis length of the review = 100**, and **padding the input sequences**. **(10 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may load the IMDB data with keras.datasets.imdb.load_data(num_words=max_features). Here. max_features is set to **1000**.\n",
        "2. You may use keras.preprocessing.sequence.pad_sequences(x_train, maxlen) to pad the input sequences and set maxlen to **100**.\n",
        "\n",
        "**Note:**\\\n",
        "We train the built LSTM-based model with ALL training data; the **validation set** (aka **development set**) is set with the **testing set** for model evaluation. This split is common in the application with limited sampled observation data, like NLP problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI4ki461S2V3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras import layers\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvV1Sv2a18SM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e5d178-4832-43ea-dfb7-d20b6ba839cd"
      },
      "source": [
        "# Prepare the data here\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=max_features) # load IMDB data with specified num_words = 1000; testing set is set to validation set.\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) # Pad IMDB training data with specified maxlen=100\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen) # Pad IMDB validation data with specified maxlen=100\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JFQeWK18SR"
      },
      "source": [
        "### 2.2 - Design and train LSTM model (25 Points) ###\n",
        "\n",
        "Build an LSTM model.\n",
        "\n",
        "**Tasks:**\n",
        "1. Build the LSTM model with **1 embedding layer**, **1 LSTM layer**, and **1 Dense layer**. **Print out** model summary. The embedding vector is specified with the dimension of **8**. **(10 Points)**\n",
        "2. Compile the LSTM model with **Adam** optimizer, **binary_crossentropy** loss function, and **accuracy** metrics. **(5 Points)**  \n",
        "3. Train the LSTM model with **batch_size=64 for 10 epochs** and report **training and validation accuracies over epochs**. **(5 Points)**\n",
        "4. **Print out** best validation accuracy. **(5 Points)**\n",
        "\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Set input dimension to **1000** and output dimension to **8** for embedding layer.\n",
        "2. Set **unit_size=8** for LSTM layer.\n",
        "3. Set activation function to **sigmoid** for Dense layer.\n",
        "4. For validation: the outputs for first epoch should be close to（but maybe not exactly following） the statistics below:\\\n",
        "- **-loss: ~0.6402 - accuracy: ~0.6187 - val_loss: ~0.4645 - val_accuracy: ~0.7995**\n",
        "5. The model summary is as follows:\n",
        "- Total params: 8,553\n",
        "- Trainable params: 8,553\n",
        "- Non-trainable params: 0\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDqqgFt118SS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173990ba-ad33-495b-d9eb-8e8611069532"
      },
      "source": [
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(1000, 8)(inputs) # Embed data in an 8-dimensional vector\n",
        "x = layers.LSTM(8)(x) # Add 1st layer of LSTM with 8 hidden states (aka units)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "m = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val)) # Train the compiled model with model.fit()\n",
        "print('\\n\\nBest validation accuracy =', max(m.history['val_accuracy']))\n",
        "\n"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 8)           8000      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 8)                 544       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,553\n",
            "Trainable params: 8,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 13s 28ms/step - loss: 0.5679 - accuracy: 0.7067 - val_loss: 0.4443 - val_accuracy: 0.8078\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 0.4148 - accuracy: 0.8166 - val_loss: 0.4015 - val_accuracy: 0.8189\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 0.3836 - accuracy: 0.8327 - val_loss: 0.3851 - val_accuracy: 0.8273\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 0.3682 - accuracy: 0.8402 - val_loss: 0.3850 - val_accuracy: 0.8258\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 0.3626 - accuracy: 0.8428 - val_loss: 0.3812 - val_accuracy: 0.8295\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 0.3597 - accuracy: 0.8427 - val_loss: 0.4304 - val_accuracy: 0.8160\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 0.3536 - accuracy: 0.8431 - val_loss: 0.3720 - val_accuracy: 0.8312\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 0.3468 - accuracy: 0.8480 - val_loss: 0.3733 - val_accuracy: 0.8312\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 0.3408 - accuracy: 0.8482 - val_loss: 0.3894 - val_accuracy: 0.8312\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 0.3358 - accuracy: 0.8523 - val_loss: 0.3825 - val_accuracy: 0.8298\n",
            "\n",
            "\n",
            "Best validation accuracy = 0.8312399983406067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vqvy2tdEw7J"
      },
      "source": [
        "### 2.3 - LSTM hyperparameter tuning (Bonus 15 Points) ###\n",
        "\n",
        "Boost the performance of obtained LSTM (aka vanilla model) by hyperparameter tuning.\n",
        "\n",
        "**Tasks:**\n",
        "Note: \n",
        "- All modificiations are directly conducted based on the vanilla model above (from 2.2).\n",
        "- For each scenario, **report <span style=\"color:red\"> BEST Validation Accuracy </span> and generate Training/Validation <span style=\"color:red\"> Accuracy plots over epochs</span>**. You may just paste the plot figures in the cells with **Markdown mode**, or leave the result after running. **Make sure it is already correctly shown in your submitted file.**\n",
        "1.  Scenario 1 (**5 points**):\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 16.\n",
        "    - Modify the units of LSTM to 16.\n",
        "2. Scenario 2 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "3. Scenario 3 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "    - Increase analysis length for review data to maxlen = 200\n",
        "\n",
        "**Hints:**  \n",
        "For validation: the outputs for first epoch should be close to （but maybe not exactly following） the statistics below:\n",
        "- Scenario 1: **loss: ~0.5839 - accuracy: ~0.6524 - val_loss: ~0.4079 - val_accuracy: ~0.8198**\n",
        "- Scenario 2: **loss: ~0.5572 - accuracy: ~0.6911 - val_loss: ~0.3953 - val_accuracy: ~0.8244**\n",
        "- Scenario 3: **loss: ~0.5605 - accuracy: ~0.6914 - val_loss: ~0.3402 - val_accuracy: ~0.8560**\n",
        "\n",
        "- Summary of Model 1: Total params: 20,241; Trainable params: 20,241; Non-trainable params: 0\n",
        "- Summary of Model 2: Total params: 391,297; Trainable params: 391,297; Non-trainable params: 0\n",
        "- Summary of Model 3: Total params: 391,297; Trainable params: 391,297; Non-trainable params: 0\n",
        "\n",
        "You may follow the example from the reference below to add additional LSTM layer.\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xMSM_GQt_P8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a8ce4e-d985-4207-87d8-7ba97ca833d2"
      },
      "source": [
        "########################### Scenario 1 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100  # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 16)(inputs) # Embed data in a 16-dimensional vector\n",
        "x = layers.LSTM(16,return_sequences=True)(x) # Add 1st layer of LSTM with 16 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(16)(x) # Add 2nd layer of LSTM with 16 hidden states (aka units)\n",
        "outputs = layers.Dense(1,activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'] ) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "m1 = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)\n",
        "print('\\n\\nBest validation accuracy =', max(m1.history['val_accuracy']))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 16)          16000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 16)          2112      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 16)                2112      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,241\n",
            "Trainable params: 20,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 25s 54ms/step - loss: 0.4968 - accuracy: 0.7450 - val_loss: 0.4079 - val_accuracy: 0.8198\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3883 - accuracy: 0.8286 - val_loss: 0.3844 - val_accuracy: 0.8242\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3688 - accuracy: 0.8382 - val_loss: 0.3692 - val_accuracy: 0.8333\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3541 - accuracy: 0.8436 - val_loss: 0.3798 - val_accuracy: 0.8257\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3446 - accuracy: 0.8464 - val_loss: 0.3665 - val_accuracy: 0.8338\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.3353 - accuracy: 0.8520 - val_loss: 0.4104 - val_accuracy: 0.8247\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3257 - accuracy: 0.8540 - val_loss: 0.3680 - val_accuracy: 0.8325\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3188 - accuracy: 0.8594 - val_loss: 0.3659 - val_accuracy: 0.8334\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3110 - accuracy: 0.8602 - val_loss: 0.3800 - val_accuracy: 0.8334\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3048 - accuracy: 0.8656 - val_loss: 0.3928 - val_accuracy: 0.8343\n",
            "\n",
            "\n",
            "Best validation accuracy = 0.8343200087547302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "NoPPDfGE9jt4",
        "outputId": "2d7d7d0b-7031-414e-aaeb-8b80ad4cb2f7"
      },
      "source": [
        "### Scenario 1 plot ####\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "x = np.arange(1,11)\n",
        "\n",
        "plt.plot(x, m1.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(x, m1.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xticks(x)\n",
        "plt.show()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keod5xXkEKnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c06c67-c4d3-45c1-aa61-73a978967b14"
      },
      "source": [
        "########################### Scenario 2 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000  # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(128, return_sequences=True)(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "m2 = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)\n",
        "print('\\n\\nBest validation accuracy =', max(m2.history['val_accuracy']))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         128000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 28s 62ms/step - loss: 0.4836 - accuracy: 0.7621 - val_loss: 0.4059 - val_accuracy: 0.8202\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 22s 56ms/step - loss: 0.3837 - accuracy: 0.8294 - val_loss: 0.3725 - val_accuracy: 0.8340\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.3576 - accuracy: 0.8423 - val_loss: 0.3782 - val_accuracy: 0.8345\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3352 - accuracy: 0.8526 - val_loss: 0.3643 - val_accuracy: 0.8376\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.3216 - accuracy: 0.8612 - val_loss: 0.3616 - val_accuracy: 0.8432\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3001 - accuracy: 0.8722 - val_loss: 0.3708 - val_accuracy: 0.8384\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.2811 - accuracy: 0.8794 - val_loss: 0.3591 - val_accuracy: 0.8362\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2686 - accuracy: 0.8869 - val_loss: 0.3645 - val_accuracy: 0.8423\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2464 - accuracy: 0.8975 - val_loss: 0.3799 - val_accuracy: 0.8338\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 21s 55ms/step - loss: 0.2279 - accuracy: 0.9074 - val_loss: 0.4102 - val_accuracy: 0.8344\n",
            "\n",
            "\n",
            "Best validation accuracy = 0.8431599736213684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Edf7OPVZA3ml",
        "outputId": "9bdee477-3b38-47b6-e759-912389efd639"
      },
      "source": [
        "### Scenario 2 plot ####\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "x = np.arange(1,11)\n",
        "\n",
        "plt.plot(x, m2.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(x, m2.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xticks(x)\n",
        "plt.show()"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdiZbuCQt_QC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71243a35-2fbd-4a7f-a800-8cc8642fda7e"
      },
      "source": [
        "########################### Scenario 3 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000  # Only consider the top 1k words\n",
        "maxlen = 200 # Only consider the first 200 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features, 128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(128, return_sequences=True)(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "m3 = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val),) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)\n",
        "print('\\n\\nBest validation accuracy =', max(m3.history['val_accuracy']))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         128000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 28s 61ms/step - loss: 0.4761 - accuracy: 0.7654 - val_loss: 0.3966 - val_accuracy: 0.8235\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3852 - accuracy: 0.8274 - val_loss: 0.3801 - val_accuracy: 0.8280\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3625 - accuracy: 0.8400 - val_loss: 0.3688 - val_accuracy: 0.8327\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3329 - accuracy: 0.8532 - val_loss: 0.3671 - val_accuracy: 0.8391\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.3134 - accuracy: 0.8638 - val_loss: 0.3570 - val_accuracy: 0.8428\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2966 - accuracy: 0.8737 - val_loss: 0.3555 - val_accuracy: 0.8457\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2790 - accuracy: 0.8816 - val_loss: 0.3588 - val_accuracy: 0.8390\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2654 - accuracy: 0.8882 - val_loss: 0.3682 - val_accuracy: 0.8399\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2449 - accuracy: 0.8989 - val_loss: 0.3953 - val_accuracy: 0.8223\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 0.2234 - accuracy: 0.9101 - val_loss: 0.4098 - val_accuracy: 0.8360\n",
            "\n",
            "\n",
            "Best validation accuracy = 0.8457199931144714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRu2xHOIt_QE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "f5aeea57-8c98-40fb-8e15-58713b8f886f"
      },
      "source": [
        "### Scenario 3 plot ####\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "x = np.arange(1,11)\n",
        "\n",
        "plt.plot(x, m3.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(x, m3.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xticks(x)\n",
        "plt.show()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}